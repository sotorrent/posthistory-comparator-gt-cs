Id;PostId;UserId;PostHistoryTypeId;RevisionGUID;CreationDate;Text;UserDisplayName;Comment
34570971;14369463;559784;2;dc74fe53-4edc-4a16-be40-e3757311f18b;2013-01-16 23:00:23.0;"I have used R's `dbinom` to generate the frequency of heads for `n=1:32` trials and plotted the graph now. It will be what you expect. I have read some of your earlier posts here on SO and on `math.stackexchange`. Still I don't understand why you'd want to `simulate` the experiment rather than generating from a binomial R.V. If you could explain it, it would be great! I'll try to work on the simulated solution from @Andrie to check out if I can match the output shown below. For now, here's something you might be interested in.&#xD;&#xA;&#xD;&#xA;    set.seed(42)&#xD;&#xA;    numbet <- 32&#xD;&#xA;    numtri <- 1e5&#xD;&#xA;    prob=5/6&#xD;&#xA;    &#xD;&#xA;    require(plyr)&#xD;&#xA;    out <- ldply(1:numbet, function(idx) {&#xD;&#xA;    	outcome <- dbinom(idx:0, size=idx, prob=prob)&#xD;&#xA;    	bet     <- rep(idx, length(outcome))&#xD;&#xA;    	N       <- round(outcome * numtri)&#xD;&#xA;    	ymin    <- c(0, head(seq_along(N)/length(N), -1))&#xD;&#xA;    	ymax    <- seq_along(N)/length(N)&#xD;&#xA;    	data.frame(bet, fill=outcome, ymin, ymax)&#xD;&#xA;    })&#xD;&#xA;    &#xD;&#xA;    require(ggplot2)&#xD;&#xA;    p <- ggplot(out, aes(xmin=bet-0.5, xmax=bet+0.5, ymin=ymin, ymax=ymax)) + &#xD;&#xA;    geom_rect(aes(fill=fill), colour=""grey80"") + &#xD;&#xA;    scale_fill_gradient(""Outcome"", low=""red"", high=""blue"") +&#xD;&#xA;    # ylim(c(0.15, 0.5)) + &#xD;&#xA;    xlab(""Bet"")&#xD;&#xA;&#xD;&#xA;`The plot:`&#xD;&#xA;&#xD;&#xA;![ggplot2][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/OguaC.png";;
34600513;14369463;559784;5;cbb6d27c-1cfc-4699-8986-5a36cf5e29c1;2013-01-17 13:22:27.0;"I have used R's `dbinom` to generate the frequency of heads for `n=1:32` trials and plotted the graph now. It will be what you expect. I have read some of your earlier posts here on SO and on `math.stackexchange`. Still I don't understand why you'd want to `simulate` the experiment rather than generating from a binomial R.V. If you could explain it, it would be great! I'll try to work on the simulated solution from @Andrie to check out if I can match the output shown below. For now, here's something you might be interested in.&#xD;&#xA;&#xD;&#xA;    set.seed(42)&#xD;&#xA;    numbet <- 32&#xD;&#xA;    numtri <- 1e5&#xD;&#xA;    prob=5/6&#xD;&#xA;    &#xD;&#xA;    require(plyr)&#xD;&#xA;    out <- ldply(1:numbet, function(idx) {&#xD;&#xA;    	outcome <- dbinom(idx:0, size=idx, prob=prob)&#xD;&#xA;    	bet     <- rep(idx, length(outcome))&#xD;&#xA;    	N       <- round(outcome * numtri)&#xD;&#xA;    	ymin    <- c(0, head(seq_along(N)/length(N), -1))&#xD;&#xA;    	ymax    <- seq_along(N)/length(N)&#xD;&#xA;    	data.frame(bet, fill=outcome, ymin, ymax)&#xD;&#xA;    })&#xD;&#xA;    &#xD;&#xA;    require(ggplot2)&#xD;&#xA;    p <- ggplot(out, aes(xmin=bet-0.5, xmax=bet+0.5, ymin=ymin, ymax=ymax)) + &#xD;&#xA;    geom_rect(aes(fill=fill), colour=""grey80"") + &#xD;&#xA;    scale_fill_gradient(""Outcome"", low=""red"", high=""blue"") +&#xD;&#xA;    xlab(""Bet"")&#xD;&#xA;&#xD;&#xA;`The plot:`&#xD;&#xA;&#xD;&#xA;![ggplot2][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/OguaC.png";;deleted 29 characters in body
34625724;14369463;559784;5;23a95afd-39a8-44a8-8275-d74f6e5be556;2013-01-17 22:22:51.0;"I have used R's `dbinom` to generate the frequency of heads for `n=1:32` trials and plotted the graph now. It will be what you expect. I have read some of your earlier posts here on SO and on `math.stackexchange`. Still I don't understand why you'd want to `simulate` the experiment rather than generating from a binomial R.V. If you could explain it, it would be great! I'll try to work on the simulated solution from @Andrie to check out if I can match the output shown below. For now, here's something you might be interested in.&#xD;&#xA;&#xD;&#xA;    set.seed(42)&#xD;&#xA;    numbet <- 32&#xD;&#xA;    numtri <- 1e5&#xD;&#xA;    prob=5/6&#xD;&#xA;    &#xD;&#xA;    require(plyr)&#xD;&#xA;    out <- ldply(1:numbet, function(idx) {&#xD;&#xA;    	outcome <- dbinom(idx:0, size=idx, prob=prob)&#xD;&#xA;    	bet     <- rep(idx, length(outcome))&#xD;&#xA;    	N       <- round(outcome * numtri)&#xD;&#xA;    	ymin    <- c(0, head(seq_along(N)/length(N), -1))&#xD;&#xA;    	ymax    <- seq_along(N)/length(N)&#xD;&#xA;    	data.frame(bet, fill=outcome, ymin, ymax)&#xD;&#xA;    })&#xD;&#xA;    &#xD;&#xA;    require(ggplot2)&#xD;&#xA;    p <- ggplot(out, aes(xmin=bet-0.5, xmax=bet+0.5, ymin=ymin, ymax=ymax)) + &#xD;&#xA;    geom_rect(aes(fill=fill), colour=""grey80"") + &#xD;&#xA;    scale_fill_gradient(""Outcome"", low=""red"", high=""blue"") +&#xD;&#xA;    xlab(""Bet"")&#xD;&#xA;&#xD;&#xA;`The plot:`&#xD;&#xA;&#xD;&#xA;![ggplot2][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/OguaC.png&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;**Edit:** Explanation of how your old code from `Andrie` works and why it doesn't give what you intend.&#xD;&#xA;&#xD;&#xA;Basically, what Andrie did was to use the idea that if you have two binomial distributions, `X ~ B(n, p)` and `Y ~ B(m, p)`, where `n, m = size` and `p = probability of success`, then, their sum, `X + Y = B(n + m, p)` (1). So, the purpose of `xcum` is to obtain the outcome for all `n = 1:32` tosses, but to explain it better, let me construct the code step by step. Along with the explanation, the code for `xcum` will also be very obvious and it can be constructed in no time (without any necessity for `for-loop` and constructing a `cumsum` everytime.&#xD;&#xA;&#xD;&#xA;If you have followed me so far, then, our idea is first to create a `numtri * numbet` matrix, with each column (`length = numtri`) having `0's` and `1's` with probability = `5/6` and `1/6` respectively. That is, if you have `numtri = 1000`, then, you'll have ~ 834 `0's` and 166 `1's` *for each of the `numbet` columns (=32 here). Let's construct this and test this first.&#xD;&#xA;&#xD;&#xA;    numtri <- 1e3&#xD;&#xA;    numbet <- 32&#xD;&#xA;    set.seed(45)&#xD;&#xA;    xcum <- t(replicate(numtri, sample(0:1, numbet, prob=c(5/6,1/6), replace = TRUE)))&#xD;&#xA;&#xD;&#xA;    # check for count of 1's&#xD;&#xA;    > apply(xcum, 2, sum)&#xD;&#xA;    [1] 169 158 166 166 160 182 164 181 168 140 154 142 169 168 159 187 176 155 151 151 166 &#xD;&#xA;    163 164 176 162 160 177 157 163 166 146 170&#xD;&#xA;&#xD;&#xA;    # So, the count of 1's are ""approximately"" what we expect (around 166).&#xD;&#xA;&#xD;&#xA;Now, each of these columns are samples of binomial distribution with `n = 1` and `size = numtri`. If we were to add the first two columns and replace the second column with this sum, then, from (1), since the probabilities are equal, we'll end up with a binomial distribution with `n = 2`. Similarly, instead, if you had added the first three columns and replaced th 3rd column by this sum, you would have obtained a binomial distribution with `n = 3` and so on...&#xD;&#xA;The concept is that if you `cumulatively` add each column, then you end up with `numbet` number of binomial distributions (1 to 32 here). So, let's do that.&#xD;&#xA;&#xD;&#xA;    xcum <- t(apply(xcum, 1, cumsum))&#xD;&#xA;    &#xD;&#xA;    # you can verify that the second column has similar probabilities by this:&#xD;&#xA;    # calculate the frequency of all values in 2nd column.&#xD;&#xA;    > table(xcum[,2])&#xD;&#xA;      0   1   2 &#xD;&#xA;    694 285  21 &#xD;&#xA;&#xD;&#xA;    > round(numtri * dbinom(2:0, 2, prob=5/6))&#xD;&#xA;    [1] 694 278  28&#xD;&#xA;    # more or less identical, good!&#xD;&#xA;&#xD;&#xA;If you divide the `xcum`, we have generated thus far by `cumsum(1:numbet)` over each row in this manner:&#xD;&#xA;&#xD;&#xA;    xcum <- xcum/matrix(rep(cumsum(1:numbet), each=numtri), ncol = numbet)&#xD;&#xA;&#xD;&#xA;This will be identical to the `xcum` matrix that comes out of the `for-loop`. I don't quite understand the reason for this. As, this is not necessary to generate the graph you require.&#xD;&#xA;&#xD;&#xA;Now on to why you have difficulties obtaining the graph I had attached (with `n+1` bins): &#xD;&#xA;&#xD;&#xA;For a binomial distribution with `n=1:32` trials, `5/6` as probability of tails (failures) and `1/6` as the probability of heads (successes), the probability of `k` heads is given by: &#xD;&#xA;&#xD;&#xA;    nCk * (5/6)^(k-1) * (1/6)^k # where nCk is n choose k&#xD;&#xA;&#xD;&#xA;For the test data we've generated, for `n=7` and `n=8` (trials), the probability of `k=0:7` and `k=0:8` heads are given by:&#xD;&#xA;&#xD;&#xA;    # n=7&#xD;&#xA;       0    1    2     3     4     5 &#xD;&#xA;    .278 .394 .233  .077  .016  .002 &#xD;&#xA;&#xD;&#xA;    # n=8&#xD;&#xA;       0    1    2    3     4      5 &#xD;&#xA;    .229 .375 .254 .111  .025   .006 &#xD;&#xA;&#xD;&#xA;Why are they both having 6 bins and not 8 and 9 bins? Of course this has to do with the value of `numtri=1000`. Let's see what's the probabilities of each of these 8 and 9 bins by generating probabilities directly from the binomial distribution using `dbinom` to understand why this happens.&#xD;&#xA;&#xD;&#xA;    # n = 7&#xD;&#xA;    dbinom(7:0, 7, prob=5/6)&#xD;&#xA;    # output rounded to 3 decimal places&#xD;&#xA;    [1] 0.279 0.391 0.234 0.078 0.016 0.002 0.000 0.000&#xD;&#xA;&#xD;&#xA;    # n = 8&#xD;&#xA;    dbinom(8:0, 8, prob=5/6)&#xD;&#xA;    # output rounded to 3 decimal places&#xD;&#xA;    [1] 0.233 0.372 0.260 0.104 0.026 0.004 0.000 0.000 0.000&#xD;&#xA;&#xD;&#xA;You see that the probabilities corresponding to k=6,7 and k=6,7,8 corresponding to n=7 and n=8 are ~ 0. They are very low in values. The minimum value here is `5.8 * 1e-7` actually (n=8, k=8). This means that you have a chance of getting 1 value if you simulated for `1/5.8 * 1e7` times. If you check the same for n=32 and k=32, the value is 1.256493 * 1e-25. So, you'll have to simulate that many values to get at least 1 result where all 32 outcomes are head for n=32. &#xD;&#xA;&#xD;&#xA;This is why your results were not having values for certain bins because the probability of having it is very low. And for the same reason, generating the probabilities directly from the binomial distribution overcomes this problem/limitation.&#xD;&#xA;&#xD;&#xA;I hope I've managed to write with enough clarity for you to follow. Let me know if you've trouble going through.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;";;added 4908 characters in body
34626318;14369463;559784;5;1d292af2-9a92-44dd-bf0a-fcba47556480;2013-01-17 22:40:52.0;"I have used R's `dbinom` to generate the frequency of heads for `n=1:32` trials and plotted the graph now. It will be what you expect. I have read some of your earlier posts here on SO and on `math.stackexchange`. Still I don't understand why you'd want to `simulate` the experiment rather than generating from a binomial R.V. If you could explain it, it would be great! I'll try to work on the simulated solution from @Andrie to check out if I can match the output shown below. For now, here's something you might be interested in.&#xD;&#xA;&#xD;&#xA;    set.seed(42)&#xD;&#xA;    numbet <- 32&#xD;&#xA;    numtri <- 1e5&#xD;&#xA;    prob=5/6&#xD;&#xA;    &#xD;&#xA;    require(plyr)&#xD;&#xA;    out <- ldply(1:numbet, function(idx) {&#xD;&#xA;    	outcome <- dbinom(idx:0, size=idx, prob=prob)&#xD;&#xA;    	bet     <- rep(idx, length(outcome))&#xD;&#xA;    	N       <- round(outcome * numtri)&#xD;&#xA;    	ymin    <- c(0, head(seq_along(N)/length(N), -1))&#xD;&#xA;    	ymax    <- seq_along(N)/length(N)&#xD;&#xA;    	data.frame(bet, fill=outcome, ymin, ymax)&#xD;&#xA;    })&#xD;&#xA;    &#xD;&#xA;    require(ggplot2)&#xD;&#xA;    p <- ggplot(out, aes(xmin=bet-0.5, xmax=bet+0.5, ymin=ymin, ymax=ymax)) + &#xD;&#xA;    geom_rect(aes(fill=fill), colour=""grey80"") + &#xD;&#xA;    scale_fill_gradient(""Outcome"", low=""red"", high=""blue"") +&#xD;&#xA;    xlab(""Bet"")&#xD;&#xA;&#xD;&#xA;`The plot:`&#xD;&#xA;&#xD;&#xA;![ggplot2][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;**Edit:** Explanation of how your old code from `Andrie` works and why it doesn't give what you intend.&#xD;&#xA;&#xD;&#xA;Basically, what Andrie did (or rather one way to look at it) is to use the idea that if you have two binomial distributions, `X ~ B(n, p)` and `Y ~ B(m, p)`, where `n, m = size` and `p = probability of success`, then, their sum, `X + Y = B(n + m, p)` (1). So, the purpose of `xcum` is to obtain the outcome for all `n = 1:32` tosses, but to explain it better, let me construct the code step by step. Along with the explanation, the code for `xcum` will also be very obvious and it can be constructed in no time (without any necessity for `for-loop` and constructing a `cumsum` everytime.&#xD;&#xA;&#xD;&#xA;If you have followed me so far, then, our idea is first to create a `numtri * numbet` matrix, with each column (`length = numtri`) having `0's` and `1's` with probability = `5/6` and `1/6` respectively. That is, if you have `numtri = 1000`, then, you'll have ~ 834 `0's` and 166 `1's` *for each of the `numbet` columns (=32 here). Let's construct this and test this first.&#xD;&#xA;&#xD;&#xA;    numtri <- 1e3&#xD;&#xA;    numbet <- 32&#xD;&#xA;    set.seed(45)&#xD;&#xA;    xcum <- t(replicate(numtri, sample(0:1, numbet, prob=c(5/6,1/6), replace = TRUE)))&#xD;&#xA;&#xD;&#xA;    # check for count of 1's&#xD;&#xA;    > apply(xcum, 2, sum)&#xD;&#xA;    [1] 169 158 166 166 160 182 164 181 168 140 154 142 169 168 159 187 176 155 151 151 166 &#xD;&#xA;    163 164 176 162 160 177 157 163 166 146 170&#xD;&#xA;&#xD;&#xA;    # So, the count of 1's are ""approximately"" what we expect (around 166).&#xD;&#xA;&#xD;&#xA;Now, each of these columns are samples of binomial distribution with `n = 1` and `size = numtri`. If we were to add the first two columns and replace the second column with this sum, then, from (1), since the probabilities are equal, we'll end up with a binomial distribution with `n = 2`. Similarly, instead, if you had added the first three columns and replaced th 3rd column by this sum, you would have obtained a binomial distribution with `n = 3` and so on...&#xD;&#xA;The concept is that if you `cumulatively` add each column, then you end up with `numbet` number of binomial distributions (1 to 32 here). So, let's do that.&#xD;&#xA;&#xD;&#xA;    xcum <- t(apply(xcum, 1, cumsum))&#xD;&#xA;    &#xD;&#xA;    # you can verify that the second column has similar probabilities by this:&#xD;&#xA;    # calculate the frequency of all values in 2nd column.&#xD;&#xA;    > table(xcum[,2])&#xD;&#xA;      0   1   2 &#xD;&#xA;    694 285  21 &#xD;&#xA;&#xD;&#xA;    > round(numtri * dbinom(2:0, 2, prob=5/6))&#xD;&#xA;    [1] 694 278  28&#xD;&#xA;    # more or less identical, good!&#xD;&#xA;&#xD;&#xA;If you divide the `xcum`, we have generated thus far by `cumsum(1:numbet)` over each row in this manner:&#xD;&#xA;&#xD;&#xA;    xcum <- xcum/matrix(rep(cumsum(1:numbet), each=numtri), ncol = numbet)&#xD;&#xA;&#xD;&#xA;this will be identical to the `xcum` matrix that comes out of the `for-loop` (if you generate it with the same seed). I don't quite understand the reason for this. This is not necessary to generate the graph you require. However, I suppose it has something to do with the `frequency` values you talked about [in an earlier post on math.stackexchange][2]&#xD;&#xA;&#xD;&#xA;Now on to why you have difficulties obtaining the graph I had attached (with `n+1` bins): &#xD;&#xA;&#xD;&#xA;For a binomial distribution with `n=1:32` trials, `5/6` as probability of tails (failures) and `1/6` as the probability of heads (successes), the probability of `k` heads is given by: &#xD;&#xA;&#xD;&#xA;    nCk * (5/6)^(k-1) * (1/6)^k # where nCk is n choose k&#xD;&#xA;&#xD;&#xA;For the test data we've generated, for `n=7` and `n=8` (trials), the probability of `k=0:7` and `k=0:8` heads are given by:&#xD;&#xA;&#xD;&#xA;    # n=7&#xD;&#xA;       0    1    2     3     4     5 &#xD;&#xA;    .278 .394 .233  .077  .016  .002 &#xD;&#xA;&#xD;&#xA;    # n=8&#xD;&#xA;       0    1    2    3     4      5 &#xD;&#xA;    .229 .375 .254 .111  .025   .006 &#xD;&#xA;&#xD;&#xA;Why are they both having 6 bins and not 8 and 9 bins? Of course this has to do with the value of `numtri=1000`. Let's see what's the probabilities of each of these 8 and 9 bins by generating probabilities directly from the binomial distribution using `dbinom` to understand why this happens.&#xD;&#xA;&#xD;&#xA;    # n = 7&#xD;&#xA;    dbinom(7:0, 7, prob=5/6)&#xD;&#xA;    # output rounded to 3 decimal places&#xD;&#xA;    [1] 0.279 0.391 0.234 0.078 0.016 0.002 0.000 0.000&#xD;&#xA;&#xD;&#xA;    # n = 8&#xD;&#xA;    dbinom(8:0, 8, prob=5/6)&#xD;&#xA;    # output rounded to 3 decimal places&#xD;&#xA;    [1] 0.233 0.372 0.260 0.104 0.026 0.004 0.000 0.000 0.000&#xD;&#xA;&#xD;&#xA;You see that the probabilities corresponding to `k=6,7` and `k=6,7,8` corresponding to `n=7` and `n=8` are ~ `0`. They are very low in values. The minimum value here is `5.8 * 1e-7` actually (`n=8`, `k=8`). This means that you have a chance of getting 1 value if you simulated for `1/5.8 * 1e7` times. If you check the same for `n=32 and k=32`, the value is `1.256493 * 1e-25`. So, you'll have to simulate that many values to get at least 1 result where all `32` outcomes are head for `n=32`. &#xD;&#xA;&#xD;&#xA;This is why your results were not having values for certain bins because the probability of having it is very low for the given `numtri`. And for the same reason, generating the probabilities directly from the binomial distribution overcomes this problem/limitation.&#xD;&#xA;&#xD;&#xA;I hope I've managed to write with enough clarity for you to follow. Let me know if you've trouble going through.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/OguaC.png&#xD;&#xA;  [2]: http://math.stackexchange.com/questions/37655/calculate-number-of-sequences-in-frequency-matrix";;added 290 characters in body
34626505;14369463;559784;5;fbf3f64f-e6b3-4978-944f-604aeb5e32e9;2013-01-17 22:46:20.0;"I have used R's `dbinom` to generate the frequency of heads for `n=1:32` trials and plotted the graph now. It will be what you expect. I have read some of your earlier posts here on SO and on `math.stackexchange`. Still I don't understand why you'd want to `simulate` the experiment rather than generating from a binomial R.V. If you could explain it, it would be great! I'll try to work on the simulated solution from @Andrie to check out if I can match the output shown below. For now, here's something you might be interested in.&#xD;&#xA;&#xD;&#xA;    set.seed(42)&#xD;&#xA;    numbet <- 32&#xD;&#xA;    numtri <- 1e5&#xD;&#xA;    prob=5/6&#xD;&#xA;    &#xD;&#xA;    require(plyr)&#xD;&#xA;    out <- ldply(1:numbet, function(idx) {&#xD;&#xA;    	outcome <- dbinom(idx:0, size=idx, prob=prob)&#xD;&#xA;    	bet     <- rep(idx, length(outcome))&#xD;&#xA;    	N       <- round(outcome * numtri)&#xD;&#xA;    	ymin    <- c(0, head(seq_along(N)/length(N), -1))&#xD;&#xA;    	ymax    <- seq_along(N)/length(N)&#xD;&#xA;    	data.frame(bet, fill=outcome, ymin, ymax)&#xD;&#xA;    })&#xD;&#xA;    &#xD;&#xA;    require(ggplot2)&#xD;&#xA;    p <- ggplot(out, aes(xmin=bet-0.5, xmax=bet+0.5, ymin=ymin, ymax=ymax)) + &#xD;&#xA;    geom_rect(aes(fill=fill), colour=""grey80"") + &#xD;&#xA;    scale_fill_gradient(""Outcome"", low=""red"", high=""blue"") +&#xD;&#xA;    xlab(""Bet"")&#xD;&#xA;&#xD;&#xA;`The plot:`&#xD;&#xA;&#xD;&#xA;![ggplot2][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;**Edit:** Explanation of how your old code from `Andrie` works and why it doesn't give what you intend.&#xD;&#xA;&#xD;&#xA;Basically, what Andrie did (or rather one way to look at it) is to use the idea that if you have two binomial distributions, `X ~ B(n, p)` and `Y ~ B(m, p)`, where `n, m = size` and `p = probability of success`, then, their sum, `X + Y = B(n + m, p)` (1). So, the purpose of `xcum` is to obtain the outcome for all `n = 1:32` tosses, but to explain it better, let me construct the code step by step. Along with the explanation, the code for `xcum` will also be very obvious and it can be constructed in no time (without any necessity for `for-loop` and constructing a `cumsum` everytime.&#xD;&#xA;&#xD;&#xA;If you have followed me so far, then, our idea is first to create a `numtri * numbet` matrix, with each column (`length = numtri`) having `0's` and `1's` with probability = `5/6` and `1/6` respectively. That is, if you have `numtri = 1000`, then, you'll have ~ 834 `0's` and 166 `1's` *for each of the `numbet` columns (=32 here). Let's construct this and test this first.&#xD;&#xA;&#xD;&#xA;    numtri <- 1e3&#xD;&#xA;    numbet <- 32&#xD;&#xA;    set.seed(45)&#xD;&#xA;    xcum <- t(replicate(numtri, sample(0:1, numbet, prob=c(5/6,1/6), replace = TRUE)))&#xD;&#xA;&#xD;&#xA;    # check for count of 1's&#xD;&#xA;    > apply(xcum, 2, sum)&#xD;&#xA;    [1] 169 158 166 166 160 182 164 181 168 140 154 142 169 168 159 187 176 155 151 151 166 &#xD;&#xA;    163 164 176 162 160 177 157 163 166 146 170&#xD;&#xA;&#xD;&#xA;    # So, the count of 1's are ""approximately"" what we expect (around 166).&#xD;&#xA;&#xD;&#xA;Now, each of these columns are samples of binomial distribution with `n = 1` and `size = numtri`. If we were to add the first two columns and replace the second column with this sum, then, from (1), since the probabilities are equal, we'll end up with a binomial distribution with `n = 2`. Similarly, instead, if you had added the first three columns and replaced th 3rd column by this sum, you would have obtained a binomial distribution with `n = 3` and so on...&#xD;&#xA;The concept is that if you `cumulatively` add each column, then you end up with `numbet` number of binomial distributions (1 to 32 here). So, let's do that.&#xD;&#xA;&#xD;&#xA;    xcum <- t(apply(xcum, 1, cumsum))&#xD;&#xA;    &#xD;&#xA;    # you can verify that the second column has similar probabilities by this:&#xD;&#xA;    # calculate the frequency of all values in 2nd column.&#xD;&#xA;    > table(xcum[,2])&#xD;&#xA;      0   1   2 &#xD;&#xA;    694 285  21 &#xD;&#xA;&#xD;&#xA;    > round(numtri * dbinom(2:0, 2, prob=5/6))&#xD;&#xA;    [1] 694 278  28&#xD;&#xA;    # more or less identical, good!&#xD;&#xA;&#xD;&#xA;If you divide the `xcum`, we have generated thus far by `cumsum(1:numbet)` over each row in this manner:&#xD;&#xA;&#xD;&#xA;    xcum <- xcum/matrix(rep(cumsum(1:numbet), each=numtri), ncol = numbet)&#xD;&#xA;&#xD;&#xA;this will be identical to the `xcum` matrix that comes out of the `for-loop` (if you generate it with the same seed). However I don't quite understand the reason for this division by Andrie as this is not necessary to generate the graph you require. However, I suppose it has something to do with the `frequency` values you talked about [in an earlier post on math.stackexchange][2]&#xD;&#xA;&#xD;&#xA;Now on to why you have difficulties obtaining the graph I had attached (with `n+1` bins): &#xD;&#xA;&#xD;&#xA;For a binomial distribution with `n=1:32` trials, `5/6` as probability of tails (failures) and `1/6` as the probability of heads (successes), the probability of `k` heads is given by: &#xD;&#xA;&#xD;&#xA;    nCk * (5/6)^(k-1) * (1/6)^k # where nCk is n choose k&#xD;&#xA;&#xD;&#xA;For the test data we've generated, for `n=7` and `n=8` (trials), the probability of `k=0:7` and `k=0:8` heads are given by:&#xD;&#xA;&#xD;&#xA;    # n=7&#xD;&#xA;       0    1    2     3     4     5 &#xD;&#xA;    .278 .394 .233  .077  .016  .002 &#xD;&#xA;&#xD;&#xA;    # n=8&#xD;&#xA;       0    1    2    3     4      5 &#xD;&#xA;    .229 .375 .254 .111  .025   .006 &#xD;&#xA;&#xD;&#xA;Why are they both having 6 bins and not 8 and 9 bins? Of course this has to do with the value of `numtri=1000`. Let's see what's the probabilities of each of these 8 and 9 bins by generating probabilities directly from the binomial distribution using `dbinom` to understand why this happens.&#xD;&#xA;&#xD;&#xA;    # n = 7&#xD;&#xA;    dbinom(7:0, 7, prob=5/6)&#xD;&#xA;    # output rounded to 3 decimal places&#xD;&#xA;    [1] 0.279 0.391 0.234 0.078 0.016 0.002 0.000 0.000&#xD;&#xA;&#xD;&#xA;    # n = 8&#xD;&#xA;    dbinom(8:0, 8, prob=5/6)&#xD;&#xA;    # output rounded to 3 decimal places&#xD;&#xA;    [1] 0.233 0.372 0.260 0.104 0.026 0.004 0.000 0.000 0.000&#xD;&#xA;&#xD;&#xA;You see that the probabilities corresponding to `k=6,7` and `k=6,7,8` corresponding to `n=7` and `n=8` are ~ `0`. They are very low in values. The minimum value here is `5.8 * 1e-7` actually (`n=8`, `k=8`). This means that you have a chance of getting 1 value if you simulated for `1/5.8 * 1e7` times. If you check the same for `n=32 and k=32`, the value is `1.256493 * 1e-25`. So, you'll have to simulate that many values to get at least 1 result where all `32` outcomes are head for `n=32`. &#xD;&#xA;&#xD;&#xA;This is why your results were not having values for certain bins because the probability of having it is very low for the given `numtri`. And for the same reason, generating the probabilities directly from the binomial distribution overcomes this problem/limitation.&#xD;&#xA;&#xD;&#xA;I hope I've managed to write with enough clarity for you to follow. Let me know if you've trouble going through.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/OguaC.png&#xD;&#xA;  [2]: http://math.stackexchange.com/questions/37655/calculate-number-of-sequences-in-frequency-matrix";;added 290 characters in body
34626824;14369463;559784;5;1756eaba-ca4f-4a0f-8e0f-8230144b0fb9;2013-01-17 22:56:11.0;"I have used R's `dbinom` to generate the frequency of heads for `n=1:32` trials and plotted the graph now. It will be what you expect. I have read some of your earlier posts here on SO and on `math.stackexchange`. Still I don't understand why you'd want to `simulate` the experiment rather than generating from a binomial R.V. If you could explain it, it would be great! I'll try to work on the simulated solution from @Andrie to check out if I can match the output shown below. For now, here's something you might be interested in.&#xD;&#xA;&#xD;&#xA;    set.seed(42)&#xD;&#xA;    numbet <- 32&#xD;&#xA;    numtri <- 1e5&#xD;&#xA;    prob=5/6&#xD;&#xA;    &#xD;&#xA;    require(plyr)&#xD;&#xA;    out <- ldply(1:numbet, function(idx) {&#xD;&#xA;    	outcome <- dbinom(idx:0, size=idx, prob=prob)&#xD;&#xA;    	bet     <- rep(idx, length(outcome))&#xD;&#xA;    	N       <- round(outcome * numtri)&#xD;&#xA;    	ymin    <- c(0, head(seq_along(N)/length(N), -1))&#xD;&#xA;    	ymax    <- seq_along(N)/length(N)&#xD;&#xA;    	data.frame(bet, fill=outcome, ymin, ymax)&#xD;&#xA;    })&#xD;&#xA;    &#xD;&#xA;    require(ggplot2)&#xD;&#xA;    p <- ggplot(out, aes(xmin=bet-0.5, xmax=bet+0.5, ymin=ymin, ymax=ymax)) + &#xD;&#xA;    geom_rect(aes(fill=fill), colour=""grey80"") + &#xD;&#xA;    scale_fill_gradient(""Outcome"", low=""red"", high=""blue"") +&#xD;&#xA;    xlab(""Bet"")&#xD;&#xA;&#xD;&#xA;`The plot:`&#xD;&#xA;&#xD;&#xA;![ggplot2][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;**Edit:** Explanation of how your old code from `Andrie` works and why it doesn't give what you intend.&#xD;&#xA;&#xD;&#xA;Basically, what Andrie did (or rather one way to look at it) is to use the idea that if you have two binomial distributions, `X ~ B(n, p)` and `Y ~ B(m, p)`, where `n, m = size` and `p = probability of success`, then, their sum, `X + Y = B(n + m, p)` (1). So, the purpose of `xcum` is to obtain the outcome for all `n = 1:32` tosses, but to explain it better, let me construct the code step by step. Along with the explanation, the code for `xcum` will also be very obvious and it can be constructed in no time (without any necessity for `for-loop` and constructing a `cumsum` everytime.&#xD;&#xA;&#xD;&#xA;If you have followed me so far, then, our idea is first to create a `numtri * numbet` matrix, with each column (`length = numtri`) having `0's` and `1's` with probability = `5/6` and `1/6` respectively. That is, if you have `numtri = 1000`, then, you'll have ~ 834 `0's` and 166 `1's` *for each of the `numbet` columns (=32 here). Let's construct this and test this first.&#xD;&#xA;&#xD;&#xA;    numtri <- 1e3&#xD;&#xA;    numbet <- 32&#xD;&#xA;    set.seed(45)&#xD;&#xA;    xcum <- t(replicate(numtri, sample(0:1, numbet, prob=c(5/6,1/6), replace = TRUE)))&#xD;&#xA;&#xD;&#xA;    # check for count of 1's&#xD;&#xA;    > apply(xcum, 2, sum)&#xD;&#xA;    [1] 169 158 166 166 160 182 164 181 168 140 154 142 169 168 159 187 176 155 151 151 166 &#xD;&#xA;    163 164 176 162 160 177 157 163 166 146 170&#xD;&#xA;&#xD;&#xA;    # So, the count of 1's are ""approximately"" what we expect (around 166).&#xD;&#xA;&#xD;&#xA;Now, each of these columns are samples of binomial distribution with `n = 1` and `size = numtri`. If we were to add the first two columns and replace the second column with this sum, then, from (1), since the probabilities are equal, we'll end up with a binomial distribution with `n = 2`. Similarly, instead, if you had added the first three columns and replaced th 3rd column by this sum, you would have obtained a binomial distribution with `n = 3` and so on...&#xD;&#xA;The concept is that if you `cumulatively` add each column, then you end up with `numbet` number of binomial distributions (1 to 32 here). So, let's do that.&#xD;&#xA;&#xD;&#xA;    xcum <- t(apply(xcum, 1, cumsum))&#xD;&#xA;    &#xD;&#xA;    # you can verify that the second column has similar probabilities by this:&#xD;&#xA;    # calculate the frequency of all values in 2nd column.&#xD;&#xA;    > table(xcum[,2])&#xD;&#xA;      0   1   2 &#xD;&#xA;    694 285  21 &#xD;&#xA;&#xD;&#xA;    > round(numtri * dbinom(2:0, 2, prob=5/6))&#xD;&#xA;    [1] 694 278  28&#xD;&#xA;    # more or less identical, good!&#xD;&#xA;&#xD;&#xA;If you divide the `xcum`, we have generated thus far by `cumsum(1:numbet)` over each row in this manner:&#xD;&#xA;&#xD;&#xA;    xcum <- xcum/matrix(rep(cumsum(1:numbet), each=numtri), ncol = numbet)&#xD;&#xA;&#xD;&#xA;this will be identical to the `xcum` matrix that comes out of the `for-loop` (if you generate it with the same seed). However I don't quite understand the reason for this division by Andrie as this is not necessary to generate the graph you require. However, I suppose it has something to do with the `frequency` values you talked about [in an earlier post on math.stackexchange][2]&#xD;&#xA;&#xD;&#xA;Now on to why you have difficulties obtaining the graph I had attached (with `n+1` bins): &#xD;&#xA;&#xD;&#xA;For a binomial distribution with `n=1:32` trials, `5/6` as probability of tails (failures) and `1/6` as the probability of heads (successes), the probability of `k` heads is given by: &#xD;&#xA;&#xD;&#xA;    nCk * (5/6)^(k-1) * (1/6)^k # where nCk is n choose k&#xD;&#xA;&#xD;&#xA;For the test data we've generated, for `n=7` and `n=8` (trials), the probability of `k=0:7` and `k=0:8` heads are given by:&#xD;&#xA;&#xD;&#xA;    # n=7&#xD;&#xA;       0    1    2     3     4     5 &#xD;&#xA;    .278 .394 .233  .077  .016  .002 &#xD;&#xA;&#xD;&#xA;    # n=8&#xD;&#xA;       0    1    2    3     4      5 &#xD;&#xA;    .229 .375 .254 .111  .025   .006 &#xD;&#xA;&#xD;&#xA;Why are they both having 6 bins and not 8 and 9 bins? Of course this has to do with the value of `numtri=1000`. Let's see what's the probabilities of each of these 8 and 9 bins by generating probabilities directly from the binomial distribution using `dbinom` to understand why this happens.&#xD;&#xA;&#xD;&#xA;    # n = 7&#xD;&#xA;    dbinom(7:0, 7, prob=5/6)&#xD;&#xA;    # output rounded to 3 decimal places&#xD;&#xA;    [1] 0.279 0.391 0.234 0.078 0.016 0.002 0.000 0.000&#xD;&#xA;&#xD;&#xA;    # n = 8&#xD;&#xA;    dbinom(8:0, 8, prob=5/6)&#xD;&#xA;    # output rounded to 3 decimal places&#xD;&#xA;    [1] 0.233 0.372 0.260 0.104 0.026 0.004 0.000 0.000 0.000&#xD;&#xA;&#xD;&#xA;You see that the probabilities corresponding to `k=6,7` and `k=6,7,8` corresponding to `n=7` and `n=8` are ~ `0`. They are very low in values. The minimum value here is `5.8 * 1e-7` actually (`n=8`, `k=8`). This means that you have a chance of getting 1 value if you simulated for `1/5.8 * 1e7` times. If you check the same for `n=32 and k=32`, the value is `1.256493 * 1e-25`. So, you'll have to simulate that many values to get at least 1 result where all `32` outcomes are head for `n=32`. &#xD;&#xA;&#xD;&#xA;This is why your results were not having values for certain bins because the probability of having it is very low for the given `numtri`. And for the same reason, generating the probabilities directly from the binomial distribution overcomes this problem/limitation.&#xD;&#xA;&#xD;&#xA;I hope I've managed to write with enough clarity for you to follow. Let me know if you've trouble going through.&#xD;&#xA;&#xD;&#xA;**Edit 2:**&#xD;&#xA;When I simulated the code I've just edited above with `numtri=1e6`, I get this for `n=7` and `n=8` and count the number of heads for `k=0:7` and `k=0:8`:&#xD;&#xA;&#xD;&#xA;    # n = 7&#xD;&#xA;         0      1      2      3      4      5      6      7 &#xD;&#xA;    279347 391386 233771  77698  15763   1915    117      3 &#xD;&#xA;&#xD;&#xA;    # n = 8&#xD;&#xA;         0      1      2      3      4      5      6      7      8 &#xD;&#xA;    232835 372466 259856 104116  26041   4271    392     22      1 &#xD;&#xA;&#xD;&#xA;Note that, there are k=6 and k=7 now for n=7 and n=8. Also, for n=8, you have a value of 1 for k=8. With increasing `numtri` you'll obtain more of the other missing bins. But it'll require a huge amount of time/memory (if at all).&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/OguaC.png&#xD;&#xA;  [2]: http://math.stackexchange.com/questions/37655/calculate-number-of-sequences-in-frequency-matrix";;added 694 characters in body
143029339;14369463;-1;5;431902da-e647-4947-a868-3d4cfc95a881;2017-04-13 12:19:19.0;"I have used R's `dbinom` to generate the frequency of heads for `n=1:32` trials and plotted the graph now. It will be what you expect. I have read some of your earlier posts here on SO and on `math.stackexchange`. Still I don't understand why you'd want to `simulate` the experiment rather than generating from a binomial R.V. If you could explain it, it would be great! I'll try to work on the simulated solution from @Andrie to check out if I can match the output shown below. For now, here's something you might be interested in.&#xD;&#xA;&#xD;&#xA;    set.seed(42)&#xD;&#xA;    numbet <- 32&#xD;&#xA;    numtri <- 1e5&#xD;&#xA;    prob=5/6&#xD;&#xA;    &#xD;&#xA;    require(plyr)&#xD;&#xA;    out <- ldply(1:numbet, function(idx) {&#xD;&#xA;    	outcome <- dbinom(idx:0, size=idx, prob=prob)&#xD;&#xA;    	bet     <- rep(idx, length(outcome))&#xD;&#xA;    	N       <- round(outcome * numtri)&#xD;&#xA;    	ymin    <- c(0, head(seq_along(N)/length(N), -1))&#xD;&#xA;    	ymax    <- seq_along(N)/length(N)&#xD;&#xA;    	data.frame(bet, fill=outcome, ymin, ymax)&#xD;&#xA;    })&#xD;&#xA;    &#xD;&#xA;    require(ggplot2)&#xD;&#xA;    p <- ggplot(out, aes(xmin=bet-0.5, xmax=bet+0.5, ymin=ymin, ymax=ymax)) + &#xD;&#xA;    geom_rect(aes(fill=fill), colour=""grey80"") + &#xD;&#xA;    scale_fill_gradient(""Outcome"", low=""red"", high=""blue"") +&#xD;&#xA;    xlab(""Bet"")&#xD;&#xA;&#xD;&#xA;`The plot:`&#xD;&#xA;&#xD;&#xA;![ggplot2][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;**Edit:** Explanation of how your old code from `Andrie` works and why it doesn't give what you intend.&#xD;&#xA;&#xD;&#xA;Basically, what Andrie did (or rather one way to look at it) is to use the idea that if you have two binomial distributions, `X ~ B(n, p)` and `Y ~ B(m, p)`, where `n, m = size` and `p = probability of success`, then, their sum, `X + Y = B(n + m, p)` (1). So, the purpose of `xcum` is to obtain the outcome for all `n = 1:32` tosses, but to explain it better, let me construct the code step by step. Along with the explanation, the code for `xcum` will also be very obvious and it can be constructed in no time (without any necessity for `for-loop` and constructing a `cumsum` everytime.&#xD;&#xA;&#xD;&#xA;If you have followed me so far, then, our idea is first to create a `numtri * numbet` matrix, with each column (`length = numtri`) having `0's` and `1's` with probability = `5/6` and `1/6` respectively. That is, if you have `numtri = 1000`, then, you'll have ~ 834 `0's` and 166 `1's` *for each of the `numbet` columns (=32 here). Let's construct this and test this first.&#xD;&#xA;&#xD;&#xA;    numtri <- 1e3&#xD;&#xA;    numbet <- 32&#xD;&#xA;    set.seed(45)&#xD;&#xA;    xcum <- t(replicate(numtri, sample(0:1, numbet, prob=c(5/6,1/6), replace = TRUE)))&#xD;&#xA;&#xD;&#xA;    # check for count of 1's&#xD;&#xA;    > apply(xcum, 2, sum)&#xD;&#xA;    [1] 169 158 166 166 160 182 164 181 168 140 154 142 169 168 159 187 176 155 151 151 166 &#xD;&#xA;    163 164 176 162 160 177 157 163 166 146 170&#xD;&#xA;&#xD;&#xA;    # So, the count of 1's are ""approximately"" what we expect (around 166).&#xD;&#xA;&#xD;&#xA;Now, each of these columns are samples of binomial distribution with `n = 1` and `size = numtri`. If we were to add the first two columns and replace the second column with this sum, then, from (1), since the probabilities are equal, we'll end up with a binomial distribution with `n = 2`. Similarly, instead, if you had added the first three columns and replaced th 3rd column by this sum, you would have obtained a binomial distribution with `n = 3` and so on...&#xD;&#xA;The concept is that if you `cumulatively` add each column, then you end up with `numbet` number of binomial distributions (1 to 32 here). So, let's do that.&#xD;&#xA;&#xD;&#xA;    xcum <- t(apply(xcum, 1, cumsum))&#xD;&#xA;    &#xD;&#xA;    # you can verify that the second column has similar probabilities by this:&#xD;&#xA;    # calculate the frequency of all values in 2nd column.&#xD;&#xA;    > table(xcum[,2])&#xD;&#xA;      0   1   2 &#xD;&#xA;    694 285  21 &#xD;&#xA;&#xD;&#xA;    > round(numtri * dbinom(2:0, 2, prob=5/6))&#xD;&#xA;    [1] 694 278  28&#xD;&#xA;    # more or less identical, good!&#xD;&#xA;&#xD;&#xA;If you divide the `xcum`, we have generated thus far by `cumsum(1:numbet)` over each row in this manner:&#xD;&#xA;&#xD;&#xA;    xcum <- xcum/matrix(rep(cumsum(1:numbet), each=numtri), ncol = numbet)&#xD;&#xA;&#xD;&#xA;this will be identical to the `xcum` matrix that comes out of the `for-loop` (if you generate it with the same seed). However I don't quite understand the reason for this division by Andrie as this is not necessary to generate the graph you require. However, I suppose it has something to do with the `frequency` values you talked about [in an earlier post on math.stackexchange][2]&#xD;&#xA;&#xD;&#xA;Now on to why you have difficulties obtaining the graph I had attached (with `n+1` bins): &#xD;&#xA;&#xD;&#xA;For a binomial distribution with `n=1:32` trials, `5/6` as probability of tails (failures) and `1/6` as the probability of heads (successes), the probability of `k` heads is given by: &#xD;&#xA;&#xD;&#xA;    nCk * (5/6)^(k-1) * (1/6)^k # where nCk is n choose k&#xD;&#xA;&#xD;&#xA;For the test data we've generated, for `n=7` and `n=8` (trials), the probability of `k=0:7` and `k=0:8` heads are given by:&#xD;&#xA;&#xD;&#xA;    # n=7&#xD;&#xA;       0    1    2     3     4     5 &#xD;&#xA;    .278 .394 .233  .077  .016  .002 &#xD;&#xA;&#xD;&#xA;    # n=8&#xD;&#xA;       0    1    2    3     4      5 &#xD;&#xA;    .229 .375 .254 .111  .025   .006 &#xD;&#xA;&#xD;&#xA;Why are they both having 6 bins and not 8 and 9 bins? Of course this has to do with the value of `numtri=1000`. Let's see what's the probabilities of each of these 8 and 9 bins by generating probabilities directly from the binomial distribution using `dbinom` to understand why this happens.&#xD;&#xA;&#xD;&#xA;    # n = 7&#xD;&#xA;    dbinom(7:0, 7, prob=5/6)&#xD;&#xA;    # output rounded to 3 decimal places&#xD;&#xA;    [1] 0.279 0.391 0.234 0.078 0.016 0.002 0.000 0.000&#xD;&#xA;&#xD;&#xA;    # n = 8&#xD;&#xA;    dbinom(8:0, 8, prob=5/6)&#xD;&#xA;    # output rounded to 3 decimal places&#xD;&#xA;    [1] 0.233 0.372 0.260 0.104 0.026 0.004 0.000 0.000 0.000&#xD;&#xA;&#xD;&#xA;You see that the probabilities corresponding to `k=6,7` and `k=6,7,8` corresponding to `n=7` and `n=8` are ~ `0`. They are very low in values. The minimum value here is `5.8 * 1e-7` actually (`n=8`, `k=8`). This means that you have a chance of getting 1 value if you simulated for `1/5.8 * 1e7` times. If you check the same for `n=32 and k=32`, the value is `1.256493 * 1e-25`. So, you'll have to simulate that many values to get at least 1 result where all `32` outcomes are head for `n=32`. &#xD;&#xA;&#xD;&#xA;This is why your results were not having values for certain bins because the probability of having it is very low for the given `numtri`. And for the same reason, generating the probabilities directly from the binomial distribution overcomes this problem/limitation.&#xD;&#xA;&#xD;&#xA;I hope I've managed to write with enough clarity for you to follow. Let me know if you've trouble going through.&#xD;&#xA;&#xD;&#xA;**Edit 2:**&#xD;&#xA;When I simulated the code I've just edited above with `numtri=1e6`, I get this for `n=7` and `n=8` and count the number of heads for `k=0:7` and `k=0:8`:&#xD;&#xA;&#xD;&#xA;    # n = 7&#xD;&#xA;         0      1      2      3      4      5      6      7 &#xD;&#xA;    279347 391386 233771  77698  15763   1915    117      3 &#xD;&#xA;&#xD;&#xA;    # n = 8&#xD;&#xA;         0      1      2      3      4      5      6      7      8 &#xD;&#xA;    232835 372466 259856 104116  26041   4271    392     22      1 &#xD;&#xA;&#xD;&#xA;Note that, there are k=6 and k=7 now for n=7 and n=8. Also, for n=8, you have a value of 1 for k=8. With increasing `numtri` you'll obtain more of the other missing bins. But it'll require a huge amount of time/memory (if at all).&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/OguaC.png&#xD;&#xA;  [2]: https://math.stackexchange.com/questions/37655/calculate-number-of-sequences-in-frequency-matrix";;replaced http://math.stackexchange.com/ with https://math.stackexchange.com/
